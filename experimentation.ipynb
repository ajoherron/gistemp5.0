{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a0545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import itertools\n",
    "import math\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8933396",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79ad207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import GHCN_temp_url, GHCN_meta_url\n",
    "GHCN_temp_url = 'https://data.giss.nasa.gov/pub/gistemp/ghcnm.tavg.qcf.dat'\n",
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "\n",
    "\n",
    "def get_GHCN_data(temp_url, meta_url):\n",
    "\n",
    "    '''\n",
    "    Retrieves and formats temperature data from the Global Historical Climatology Network (GHCN) dataset.\n",
    "\n",
    "    Args:\n",
    "    temp_url (str): The URL to the temperature data file in GHCN format.\n",
    "    meta_url (str): The URL to the metadata file containing station information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing temperature data with station metadata.\n",
    "    \n",
    "    This function sends an HTTP GET request to the temperature data URL, processes the data to create\n",
    "    a formatted DataFrame, replaces missing values with NaN, converts temperature values to degrees Celsius,\n",
    "    and merges the data with station metadata based on station IDs. The resulting DataFrame includes\n",
    "    columns for station latitude, longitude, and name, and is indexed by station IDs.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(temp_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Get the content of the response\n",
    "            file_data = response.content.decode(\"utf-8\")\n",
    "\n",
    "            # Create a list to store formatted data\n",
    "            formatted_data = []\n",
    "\n",
    "            # Loop through file data\n",
    "            for line in file_data.split('\\n'):\n",
    "                \n",
    "                # Check if line is not empty\n",
    "                if line.strip():\n",
    "                    \n",
    "                    # Extract relevant data\n",
    "                    # (Using code from GHCNV4Reader())\n",
    "                    station_id = line[:11]\n",
    "                    year = int(line[11:15])\n",
    "                    values = [int(line[i:i+5]) for i in range(19, 115, 8)]\n",
    "                    \n",
    "                    # Append data to list\n",
    "                    formatted_data.append([station_id, year] + values)\n",
    "\n",
    "            # Create DataFrame from formatted data\n",
    "            column_names = ['Station_ID', 'Year'] + [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN = pd.DataFrame(formatted_data, columns=column_names)\n",
    "            \n",
    "            # Replace -9999 with NaN\n",
    "            df_GHCN.replace(-9999, np.nan, inplace=True)\n",
    "            \n",
    "            # Format data - convert to degrees C\n",
    "            month_columns = [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN[month_columns] = df_GHCN[month_columns].divide(100)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "    # Define the column widths, create meta data dataframe\n",
    "    column_widths = [11, 9, 10, 7, 3, 31]\n",
    "    df_meta = pd.read_fwf(meta_url, widths=column_widths, header=None,\n",
    "                          names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "    # Merge on station ID, set index\n",
    "    df = pd.merge(df_GHCN, df_meta[['Station_ID', 'Latitude', 'Longitude', 'Name']], on='Station_ID', how='left')\n",
    "    df = df.set_index('Station_ID')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def step0():\n",
    "    '''\n",
    "    Performs the initial data processing steps for the GHCN temperature dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing filtered and formatted temperature data.\n",
    "    \n",
    "    This function retrieves temperature data from the Global Historical Climatology Network (GHCN) dataset,\n",
    "    processes and formats the data, and returns a DataFrame. The data is first fetched using specified URLs,\n",
    "    and is returned for further analysis.\n",
    "    '''\n",
    "    df_GHCN = get_GHCN_data(GHCN_temp_url, GHCN_meta_url)\n",
    "    return df_GHCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0843ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "step0_output = step0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae85db2",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2773a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import drop_rules\n",
    "drop_rules = '''\n",
    "CHM00052836  omit: 0-1948\n",
    "CHXLT909860  omit: 0-1950\n",
    "BL000085365  omit: 0-1930\n",
    "MXXLT948335  omit: 0-1952\n",
    "ASN00058012  omit: 0-1899\n",
    "ASN00084016  omit: 0-1899\n",
    "ASN00069018  omit: 0-1898\n",
    "NIXLT013080  omit: 0-1930\n",
    "NIXLT751359  omit: 0-9999\n",
    "CHXLT063941  omit: 0-1937\n",
    "CHM00054843  omit: 0-1937\n",
    "MXM00076373  omit: 0-9999\n",
    "USC00044022  omit: 0-9999\n",
    "USC00044025  omit: 0-9999\n",
    "CA002402332  omit: 2011-9999\n",
    "RSM00024266  omit: 2021/09\n",
    "'''\n",
    "\n",
    "\n",
    "def filter_coordinates(df):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on latitude and longitude conditions.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame with rows where latitude is between -90 and 90,\n",
    "    and longitude is between -180 and 180.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define latitude and longitude range conditions\n",
    "    lat_condition = (df['Latitude'] >= -90) & (df['Latitude'] <= 90)\n",
    "    lon_condition = (df['Longitude'] >= -180) & (df['Longitude'] <= 180)\n",
    "\n",
    "    # Apply the conditions to filter the DataFrame\n",
    "    df_filtered = df[lat_condition & lon_condition]\n",
    "    \n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(df) - len(df_filtered)\n",
    "    print(f'Number of rows with invalid coordinates (removed): {num_filtered}')\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_stations_by_rules(dataframe, rules_text):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame of climate station data based on exclusion rules specified in a text format.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing climate station data.\n",
    "        rules_text (str): A string containing exclusion rules for specific stations and years.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with stations omitted based on the provided rules.\n",
    "\n",
    "    Rules Format:\n",
    "        The 'rules_text' should be formatted as follows:\n",
    "        - Each rule is represented as a single line in the text.\n",
    "        - Each line should start with the station ID followed by exclusion rules.\n",
    "        - Exclusion rules consist of 'omit:' followed by the years to exclude, e.g., 'omit: 2000-2010'.\n",
    "        - Years can be specified as a single year (e.g., 'omit: 2000') or as a range (e.g., 'omit: 2000-2010').\n",
    "        - Year ranges can also be specified using '/' (e.g., 'omit: 2000/2002').\n",
    "\n",
    "    Example:\n",
    "        rules_text = '''\n",
    "            CHM00052836  omit: 0-1948\n",
    "            CHXLT909860  omit: 0-1950\n",
    "            BL000085365  omit: 0-1930\n",
    "            ...\n",
    "        '''\n",
    "\n",
    "    This function takes the provided rules and applies them to the input DataFrame,\n",
    "    resulting in a new DataFrame with stations excluded based on the specified rules.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the rules from the provided text\n",
    "    rules = {}\n",
    "    for line in rules_text.split('\\n'):\n",
    "        if line.strip():\n",
    "            match = re.match(r'([A-Z0-9]+)\\s+omit:\\s+(\\S+)', line)\n",
    "            if match:\n",
    "                station_id, year_rule = match.groups()\n",
    "                rules[station_id] = year_rule\n",
    "\n",
    "    # Create a mask to identify rows to omit\n",
    "    mask = pd.Series(True, index=dataframe.index)\n",
    "\n",
    "    for station_id, year_rule in rules.items():\n",
    "        try:\n",
    "            # Split the year_rule into start and end years\n",
    "            start_year, end_year = map(int, year_rule.split('-'))\n",
    "        except ValueError:\n",
    "            # Handle cases like '2011/12' or '2012-9999'\n",
    "            if '/' in year_rule:\n",
    "                start_year = int(year_rule.split('/')[0])\n",
    "                end_year = start_year\n",
    "            elif '-' in year_rule:\n",
    "                start_year = int(year_rule.split('-')[0])\n",
    "                end_year = int(year_rule.split('-')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Update the mask to False for the specified range of years for the station_id\n",
    "        mask &= ~((dataframe['Year'] >= start_year) & (dataframe['Year'] <= end_year) & (dataframe.index == station_id))\n",
    "\n",
    "    # Apply the mask to filter the DataFrame\n",
    "    filtered_dataframe = dataframe[mask]\n",
    "\n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(dataframe) - len(filtered_dataframe)\n",
    "    print(f'Number of rows removed according to station exclusion rules: {num_filtered}')\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "\n",
    "def step1(step0_output):\n",
    "    \"\"\"\n",
    "    Applies data filtering and cleaning operations to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        step0_output (pd.DataFrame): The initial DataFrame containing climate station data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame ready for further analysis.\n",
    "\n",
    "    This function serves as a data processing step by applying two essential filtering operations:\n",
    "    1. `filter_coordinates`: Filters the DataFrame based on geographical coordinates, retaining relevant stations.\n",
    "    2. `filter_stations_by_rules`: Filters the DataFrame based on exclusion rules, omitting specified stations and years.\n",
    "\n",
    "    The resulting DataFrame is cleaned of irrelevant stations and years according to specified rules\n",
    "    and is ready for subsequent data analysis or visualization.\n",
    "    \"\"\"\n",
    "        \n",
    "    df_filtered = filter_coordinates(step0_output)\n",
    "    df_clean = filter_stations_by_rules(df_filtered, drop_rules)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a47d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with invalid coordinates (removed): 194947\n",
      "Number of rows removed according to station exclusion rules: 344\n"
     ]
    }
   ],
   "source": [
    "step1_output = step1(step0_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601a9e5",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7512e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313754b",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0770b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_80_cell_grid():\n",
    "    # Define the custom latitudinal bands\n",
    "    lat_bands = [-90, -64.2, -44.4, -23.6, 0, 23.6, 44.4, 64.2, 90]\n",
    "    n_bands = len(lat_bands)\n",
    "\n",
    "    n_boxes_per_band = 10  # Number of boxes per band\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for band in range(n_bands - 1):\n",
    "        lat_south = lat_bands[band]\n",
    "        lat_north = lat_bands[band + 1]\n",
    "\n",
    "        for i in range(n_boxes_per_band):\n",
    "            lon_west = -180 + i * (360 / n_boxes_per_band)\n",
    "            lon_east = -180 + (i + 1) * (360 / n_boxes_per_band)\n",
    "\n",
    "            data.append((lat_south, lat_north, lon_west, lon_east))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "\n",
    "    # Calculate the center latitude and longitude\n",
    "    df['Center_Latitude'] = (df['Southern'] + df['Northern']) / 2\n",
    "    df['Center_Longitude'] = (df['Western'] + df['Eastern']) / 2\n",
    "\n",
    "    return df\n",
    "\n",
    "def lerp(x, y, p):\n",
    "    return y * p + (1 - p) * x\n",
    "\n",
    "# def generate_8000_cell_grid():\n",
    "#     def subgen(lat_s, lat_n, lon_w, lon_e):\n",
    "#         alts = math.sin(lat_s * math.pi / 180)\n",
    "#         altn = math.sin(lat_n * math.pi / 180)\n",
    "#         for y in range(10):\n",
    "#             s = 180 * math.asin(lerp(alts, altn, y * 0.1)) / math.pi\n",
    "#             n = 180 * math.asin(lerp(alts, altn, (y + 1) * 0.1)) / math.pi\n",
    "#             for x in range(10):\n",
    "#                 w = lerp(lon_w, lon_e, x * 0.1)\n",
    "#                 e = lerp(lon_w, lon_e, (x + 1) * 0.1)\n",
    "#                 yield (s, n, w, e)\n",
    "\n",
    "#     initial_regions_df = generate_80_cell_grid()\n",
    "#     data = []\n",
    "\n",
    "#     for index, row in initial_regions_df.iterrows():\n",
    "#         for subcell in subgen(row['Southern'], row['Northern'], row['Western'], row['Eastern']):\n",
    "#             data.append(subcell)\n",
    "\n",
    "#     grid_df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "    \n",
    "#     # Calculate the center latitude and longitude\n",
    "#     grid_df['Center_Latitude'] = (grid_df['Southern'] + grid_df['Northern']) / 2\n",
    "#     grid_df['Center_Longitude'] = (grid_df['Western'] + grid_df['Eastern']) / 2\n",
    "    \n",
    "#     return grid_df\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the spherical distance (in kilometers) between two pairs of\n",
    "    latitude and longitude coordinates using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Spherical distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    radius = 6371.0  # Earth's mean radius\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def linearly_decreasing_weight(distance, max_distance):\n",
    "    \"\"\"\n",
    "    Calculate a linearly decreasing weight based on the given distance\n",
    "    and maximum distance.\n",
    "\n",
    "    Args:\n",
    "        distance (float): The distance at which you want to calculate the weight.\n",
    "        max_distance (float): The maximum distance at which the weight becomes 0.\n",
    "\n",
    "    Returns:\n",
    "        float: The linearly decreasing weight, ranging from 1 to 0.\n",
    "    \"\"\"\n",
    "    # Ensure that distance is within the valid range [0, max_distance]\n",
    "    distance = max(0, min(distance, max_distance))\n",
    "\n",
    "    # Calculate the weight as a linear interpolation\n",
    "    weight = 1.0 - (distance / max_distance)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88c71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect meta data for stations\n",
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "column_widths = [11, 9, 10, 7, 3, 31]\n",
    "station_df = pd.read_fwf(GHCN_meta_url, widths=column_widths, header=None,\n",
    "                      names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "# Create 80 cell grid\n",
    "grid_80_df = generate_80_cell_grid()\n",
    "\n",
    "# Initialize an empty list to store station IDs and weights as dictionaries\n",
    "station_weights_within_radius = []\n",
    "\n",
    "# Maximum distance for the weight calculation (e.g., 1200.0 km)\n",
    "max_distance = 1200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a812cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 80/80 [00:28<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Southern  Northern  Western  Eastern  Center_Latitude  Center_Longitude  \\\n",
      "0      -90.0     -64.2   -180.0   -144.0            -77.1            -162.0   \n",
      "1      -90.0     -64.2   -144.0   -108.0            -77.1            -126.0   \n",
      "2      -90.0     -64.2   -108.0    -72.0            -77.1             -90.0   \n",
      "3      -90.0     -64.2    -72.0    -36.0            -77.1             -54.0   \n",
      "4      -90.0     -64.2    -36.0      0.0            -77.1             -18.0   \n",
      "..       ...       ...      ...      ...              ...               ...   \n",
      "75      64.2      90.0      0.0     36.0             77.1              18.0   \n",
      "76      64.2      90.0     36.0     72.0             77.1              54.0   \n",
      "77      64.2      90.0     72.0    108.0             77.1              90.0   \n",
      "78      64.2      90.0    108.0    144.0             77.1             126.0   \n",
      "79      64.2      90.0    144.0    180.0             77.1             162.0   \n",
      "\n",
      "                                      Nearby_Stations  \n",
      "0   {'AYM00089314': 0.07869935781270576, 'AYM00089...  \n",
      "1   {'AYM00089087': 0.11183970882868, 'AYM00089314...  \n",
      "2   {'AYM00089065': 0.24235042294671494, 'AYM00089...  \n",
      "3   {'AYM00089013': 0.1847660848038779, 'AYM000890...  \n",
      "4   {'AYM00089001': 0.25517982297706665, 'AYM00089...  \n",
      "..                                                ...  \n",
      "75  {'FI000007501': 0.06811829575338513, 'FIE00146...  \n",
      "76  {'FIE00146698': 0.030788945747714958, 'FIE0014...  \n",
      "77  {'RSM00020046': 0.35346151056284025, 'RSM00020...  \n",
      "78  {'RSM00020069': 0.08294621945368075, 'RSM00020...  \n",
      "79  {'RSM00021358': 0.7849798937727188, 'RSM000214...  \n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Collect meta data for stations\n",
    "# GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "# column_widths = [11, 9, 10, 7, 3, 31]\n",
    "# station_df = pd.read_fwf(GHCN_meta_url, widths=column_widths, header=None,\n",
    "#                       names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "# # Create 80 cell grid\n",
    "# grid_80_df = generate_80_cell_grid()\n",
    "\n",
    "# # Initialize an empty list to store station IDs and weights as dictionaries\n",
    "# station_weights_within_radius = []\n",
    "\n",
    "# # Maximum distance for the weight calculation (e.g., 1200.0 km)\n",
    "# max_distance = 1200.0\n",
    "\n",
    "# Use tqdm to track progress\n",
    "for index, row in tqdm(grid_80_df.iterrows(), total=len(grid_80_df), desc=\"Processing\"):\n",
    "    center_lat = row['Center_Latitude']\n",
    "    center_lon = row['Center_Longitude']\n",
    "    \n",
    "    # Calculate distances for each station in station_df\n",
    "    distances = station_df.apply(lambda x: haversine_distance(center_lat, center_lon, x['Latitude'], x['Longitude']), axis=1)\n",
    "    \n",
    "    # Find station IDs within the specified radius\n",
    "    nearby_stations = station_df[distances <= max_distance]\n",
    "    \n",
    "    # Calculate weights for each nearby station\n",
    "    weights = nearby_stations.apply(lambda x: linearly_decreasing_weight(distances[x.name], max_distance), axis=1)\n",
    "    \n",
    "    # Create a dictionary of station IDs and weights\n",
    "    station_weights = dict(zip(nearby_stations['Station_ID'], weights))\n",
    "    \n",
    "    # Append the dictionary to the result list\n",
    "    station_weights_within_radius.append(station_weights)\n",
    "\n",
    "# Add the list of station IDs and weights as a new column\n",
    "grid_80_df['Nearby_Stations'] = station_weights_within_radius\n",
    "\n",
    "# Print grid_80_df with the new column\n",
    "print(grid_80_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c1995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AYM00089314': 0.07869935781270576,\n",
       " 'AYM00089324': 0.19365261976703352,\n",
       " 'AYM00089327': 0.1120674332336622,\n",
       " 'AYM00089329': 0.1817848329568028,\n",
       " 'AYM00089332': 0.3579863036246931,\n",
       " 'AYM00089345': 0.5243641062018085,\n",
       " 'AYM00089376': 0.5966787562789673,\n",
       " 'AYM00089377': 0.46096272224883406,\n",
       " 'AYM00089659': 0.08761266665748224,\n",
       " 'AYM00089661': 0.26762380970394817,\n",
       " 'AYM00089662': 0.21571836096846975,\n",
       " 'AYM00089664': 0.3759771446164176,\n",
       " 'AYM00089666': 0.2747499921490105,\n",
       " 'AYM00089667': 0.37243267931071955,\n",
       " 'AYM00089768': 0.38093618171015087,\n",
       " 'AYM00089769': 0.41250410664085735,\n",
       " 'AYM00089864': 0.21920446453615938,\n",
       " 'AYM00089865': 0.36549004548633346,\n",
       " 'AYM00089868': 0.4287980578846665,\n",
       " 'AYM00089869': 0.35027610809683973,\n",
       " 'AYM00089872': 0.45489659407733074,\n",
       " 'AYM00089873': 0.33582961948846224,\n",
       " 'AYM00089879': 0.19056935957954546,\n",
       " 'AYW00067401': 0.6616238320823495,\n",
       " 'AYW00067402': 0.681939039398793,\n",
       " 'AYW00067601': 0.8979511993001481,\n",
       " 'AYW00068201': 0.19586618898123764,\n",
       " 'AYW00068701': 0.5974361687102907,\n",
       " 'AYW00087602': 0.30804061546357697,\n",
       " 'AYW00087701': 0.19997012526364022,\n",
       " 'AYW00088702': 0.3228424428591439,\n",
       " 'AYW00088703': 0.394089947508969}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_80_df.iloc[0]['Nearby_Stations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a55c8",
   "metadata": {},
   "source": [
    "# Xarray Conversion (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = step1_output\n",
    "\n",
    "# # Transpose the DataFrame to have months as columns\n",
    "# df_copy = df.set_index(['Year', 'Name', 'Latitude', 'Longitude']).transpose()\n",
    "\n",
    "# # Create a MultiIndex with separate levels for Station_ID, Year, and Month\n",
    "# df_copy.columns = pd.MultiIndex.from_tuples(\n",
    "#     [(station_id, year, f\"Month_{month}\") for station_id in df_copy.columns.get_level_values(0) for year in df_copy.index.get_level_values(0) for month in range(1, 13)],\n",
    "#     names=['Station_ID', 'Year', 'Month']\n",
    "# )\n",
    "\n",
    "# # Drop Year column from the copied DataFrame\n",
    "# df_copy = df_copy.drop('Year', axis=0)\n",
    "\n",
    "# # Convert the copied DataFrame to an xarray Dataset\n",
    "# ds = xr.Dataset.from_dataframe(df_copy)\n",
    "\n",
    "# # Rename latitude and longitude variables (if needed)\n",
    "# ds = ds.rename({'Longitude': 'Lon', 'Latitude': 'Lat'})\n",
    "\n",
    "# # Print the resulting xarray Dataset\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee5e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
