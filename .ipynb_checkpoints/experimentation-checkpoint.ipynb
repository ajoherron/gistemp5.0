{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a0545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import itertools\n",
    "import math\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2cbc0",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd898bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import GHCN_temp_url, GHCN_meta_url\n",
    "GHCN_temp_url = 'https://data.giss.nasa.gov/pub/gistemp/ghcnm.tavg.qcf.dat'\n",
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "\n",
    "\n",
    "def get_GHCN_data(temp_url, meta_url):\n",
    "\n",
    "    '''\n",
    "    Retrieves and formats temperature data from the Global Historical Climatology Network (GHCN) dataset.\n",
    "\n",
    "    Args:\n",
    "    temp_url (str): The URL to the temperature data file in GHCN format.\n",
    "    meta_url (str): The URL to the metadata file containing station information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing temperature data with station metadata.\n",
    "    \n",
    "    This function sends an HTTP GET request to the temperature data URL, processes the data to create\n",
    "    a formatted DataFrame, replaces missing values with NaN, converts temperature values to degrees Celsius,\n",
    "    and merges the data with station metadata based on station IDs. The resulting DataFrame includes\n",
    "    columns for station latitude, longitude, and name, and is indexed by station IDs.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(temp_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Get the content of the response\n",
    "            file_data = response.content.decode(\"utf-8\")\n",
    "\n",
    "            # Create a list to store formatted data\n",
    "            formatted_data = []\n",
    "\n",
    "            # Loop through file data\n",
    "            for line in file_data.split('\\n'):\n",
    "                \n",
    "                # Check if line is not empty\n",
    "                if line.strip():\n",
    "                    \n",
    "                    # Extract relevant data\n",
    "                    # (Using code from GHCNV4Reader())\n",
    "                    station_id = line[:11]\n",
    "                    year = int(line[11:15])\n",
    "                    values = [int(line[i:i+5]) for i in range(19, 115, 8)]\n",
    "                    \n",
    "                    # Append data to list\n",
    "                    formatted_data.append([station_id, year] + values)\n",
    "\n",
    "            # Create DataFrame from formatted data\n",
    "            column_names = ['Station_ID', 'Year'] + [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN = pd.DataFrame(formatted_data, columns=column_names)\n",
    "            \n",
    "            # Replace -9999 with NaN\n",
    "            df_GHCN.replace(-9999, np.nan, inplace=True)\n",
    "            \n",
    "            # Format data - convert to degrees C\n",
    "            month_columns = [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN[month_columns] = df_GHCN[month_columns].divide(100)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "    # Define the column widths, create meta data dataframe\n",
    "    column_widths = [11, 9, 10, 7, 3, 31]\n",
    "    df_meta = pd.read_fwf(meta_url, widths=column_widths, header=None,\n",
    "                          names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "    # Merge on station ID, set index\n",
    "    df = pd.merge(df_GHCN, df_meta[['Station_ID', 'Latitude', 'Longitude', 'Name']], on='Station_ID', how='left')\n",
    "    df = df.set_index('Station_ID')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def step0():\n",
    "    '''\n",
    "    Performs the initial data processing steps for the GHCN temperature dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing filtered and formatted temperature data.\n",
    "    \n",
    "    This function retrieves temperature data from the Global Historical Climatology Network (GHCN) dataset,\n",
    "    processes and formats the data, and returns a DataFrame. The data is first fetched using specified URLs,\n",
    "    and is returned for further analysis.\n",
    "    '''\n",
    "    df_GHCN = get_GHCN_data(GHCN_temp_url, GHCN_meta_url)\n",
    "    return df_GHCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50153f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "step0_output = step0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f45574",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cdc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import drop_rules\n",
    "drop_rules = '''\n",
    "CHM00052836  omit: 0-1948\n",
    "CHXLT909860  omit: 0-1950\n",
    "BL000085365  omit: 0-1930\n",
    "MXXLT948335  omit: 0-1952\n",
    "ASN00058012  omit: 0-1899\n",
    "ASN00084016  omit: 0-1899\n",
    "ASN00069018  omit: 0-1898\n",
    "NIXLT013080  omit: 0-1930\n",
    "NIXLT751359  omit: 0-9999\n",
    "CHXLT063941  omit: 0-1937\n",
    "CHM00054843  omit: 0-1937\n",
    "MXM00076373  omit: 0-9999\n",
    "USC00044022  omit: 0-9999\n",
    "USC00044025  omit: 0-9999\n",
    "CA002402332  omit: 2011-9999\n",
    "RSM00024266  omit: 2021/09\n",
    "'''\n",
    "\n",
    "\n",
    "def filter_coordinates(df):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on latitude and longitude conditions.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame with rows where latitude is between -90 and 90,\n",
    "    and longitude is between -180 and 180.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define latitude and longitude range conditions\n",
    "    lat_condition = (df['Latitude'] >= -90) & (df['Latitude'] <= 90)\n",
    "    lon_condition = (df['Longitude'] >= -180) & (df['Longitude'] <= 180)\n",
    "\n",
    "    # Apply the conditions to filter the DataFrame\n",
    "    df_filtered = df[lat_condition & lon_condition]\n",
    "    \n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(df) - len(df_filtered)\n",
    "    print(f'Number of rows with invalid coordinates (removed): {num_filtered}')\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_stations_by_rules(dataframe, rules_text):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame of climate station data based on exclusion rules specified in a text format.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing climate station data.\n",
    "        rules_text (str): A string containing exclusion rules for specific stations and years.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with stations omitted based on the provided rules.\n",
    "\n",
    "    Rules Format:\n",
    "        The 'rules_text' should be formatted as follows:\n",
    "        - Each rule is represented as a single line in the text.\n",
    "        - Each line should start with the station ID followed by exclusion rules.\n",
    "        - Exclusion rules consist of 'omit:' followed by the years to exclude, e.g., 'omit: 2000-2010'.\n",
    "        - Years can be specified as a single year (e.g., 'omit: 2000') or as a range (e.g., 'omit: 2000-2010').\n",
    "        - Year ranges can also be specified using '/' (e.g., 'omit: 2000/2002').\n",
    "\n",
    "    Example:\n",
    "        rules_text = '''\n",
    "            CHM00052836  omit: 0-1948\n",
    "            CHXLT909860  omit: 0-1950\n",
    "            BL000085365  omit: 0-1930\n",
    "            ...\n",
    "        '''\n",
    "\n",
    "    This function takes the provided rules and applies them to the input DataFrame,\n",
    "    resulting in a new DataFrame with stations excluded based on the specified rules.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the rules from the provided text\n",
    "    rules = {}\n",
    "    for line in rules_text.split('\\n'):\n",
    "        if line.strip():\n",
    "            match = re.match(r'([A-Z0-9]+)\\s+omit:\\s+(\\S+)', line)\n",
    "            if match:\n",
    "                station_id, year_rule = match.groups()\n",
    "                rules[station_id] = year_rule\n",
    "\n",
    "    # Create a mask to identify rows to omit\n",
    "    mask = pd.Series(True, index=dataframe.index)\n",
    "\n",
    "    for station_id, year_rule in rules.items():\n",
    "        try:\n",
    "            # Split the year_rule into start and end years\n",
    "            start_year, end_year = map(int, year_rule.split('-'))\n",
    "        except ValueError:\n",
    "            # Handle cases like '2011/12' or '2012-9999'\n",
    "            if '/' in year_rule:\n",
    "                start_year = int(year_rule.split('/')[0])\n",
    "                end_year = start_year\n",
    "            elif '-' in year_rule:\n",
    "                start_year = int(year_rule.split('-')[0])\n",
    "                end_year = int(year_rule.split('-')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Update the mask to False for the specified range of years for the station_id\n",
    "        mask &= ~((dataframe['Year'] >= start_year) & (dataframe['Year'] <= end_year) & (dataframe.index == station_id))\n",
    "\n",
    "    # Apply the mask to filter the DataFrame\n",
    "    filtered_dataframe = dataframe[mask]\n",
    "\n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(dataframe) - len(filtered_dataframe)\n",
    "    print(f'Number of rows removed according to station exclusion rules: {num_filtered}')\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "\n",
    "def step1(step0_output):\n",
    "    \"\"\"\n",
    "    Applies data filtering and cleaning operations to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        step0_output (pd.DataFrame): The initial DataFrame containing climate station data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame ready for further analysis.\n",
    "\n",
    "    This function serves as a data processing step by applying two essential filtering operations:\n",
    "    1. `filter_coordinates`: Filters the DataFrame based on geographical coordinates, retaining relevant stations.\n",
    "    2. `filter_stations_by_rules`: Filters the DataFrame based on exclusion rules, omitting specified stations and years.\n",
    "\n",
    "    The resulting DataFrame is cleaned of irrelevant stations and years according to specified rules\n",
    "    and is ready for subsequent data analysis or visualization.\n",
    "    \"\"\"\n",
    "        \n",
    "    df_filtered = filter_coordinates(step0_output)\n",
    "    df_clean = filter_stations_by_rules(df_filtered, drop_rules)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45331c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with invalid coordinates (removed): 194947\n",
      "Number of rows removed according to station exclusion rules: 344\n"
     ]
    }
   ],
   "source": [
    "step1_output = step1(step0_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af637ac6",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34497543",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee342010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_80_cell_grid():\n",
    "    lat_bands = [-90, -64.2, -44.4, -23.6, 0, 23.6, 44.4, 64.2, 90]\n",
    "    n_bands = len(lat_bands) - 1  # Number of latitude bands\n",
    "    n_boxes_per_band = 10  # Number of boxes per band\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for band in range(n_bands):\n",
    "        lat_south = lat_bands[band]\n",
    "        lat_north = lat_bands[band + 1]\n",
    "\n",
    "        for i in range(n_boxes_per_band):\n",
    "            lon_west = -180 + i * (360 / n_boxes_per_band)\n",
    "            lon_east = -180 + (i + 1) * (360 / n_boxes_per_band)\n",
    "\n",
    "            # Calculate the equal area center latitude and longitude\n",
    "            sinc = 0.5 * (math.sin(lat_south * math.pi / 180) + math.sin(lat_north * math.pi / 180))\n",
    "            center_latitude = math.asin(sinc) * 180 / math.pi\n",
    "            center_longitude = 0.5 * (lon_west + lon_east)\n",
    "\n",
    "            data.append((lat_south, lat_north, lon_west, lon_east, center_latitude, center_longitude))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern', 'Center_Latitude', 'Center_Longitude'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def lerp(x, y, p):\n",
    "    return y * p + (1 - p) * x\n",
    "\n",
    "# def generate_8000_cell_grid():\n",
    "#     def subgen(lat_s, lat_n, lon_w, lon_e):\n",
    "#         alts = math.sin(lat_s * math.pi / 180)\n",
    "#         altn = math.sin(lat_n * math.pi / 180)\n",
    "#         for y in range(10):\n",
    "#             s = 180 * math.asin(lerp(alts, altn, y * 0.1)) / math.pi\n",
    "#             n = 180 * math.asin(lerp(alts, altn, (y + 1) * 0.1)) / math.pi\n",
    "#             for x in range(10):\n",
    "#                 w = lerp(lon_w, lon_e, x * 0.1)\n",
    "#                 e = lerp(lon_w, lon_e, (x + 1) * 0.1)\n",
    "#                 yield (s, n, w, e)\n",
    "\n",
    "#     initial_regions_df = generate_80_cell_grid()\n",
    "#     data = []\n",
    "\n",
    "#     for index, row in initial_regions_df.iterrows():\n",
    "#         for subcell in subgen(row['Southern'], row['Northern'], row['Western'], row['Eastern']):\n",
    "#             data.append(subcell)\n",
    "\n",
    "#     grid_df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "    \n",
    "#     # Calculate the center latitude and longitude\n",
    "#     grid_df['Center_Latitude'] = (grid_df['Southern'] + grid_df['Northern']) / 2\n",
    "#     grid_df['Center_Longitude'] = (grid_df['Western'] + grid_df['Eastern']) / 2\n",
    "    \n",
    "#     return grid_df\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the spherical distance (in kilometers) between two pairs of\n",
    "    latitude and longitude coordinates using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Spherical distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    radius = 6371.0  # Earth's mean radius\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def linearly_decreasing_weight(distance, max_distance):\n",
    "    \"\"\"\n",
    "    Calculate a linearly decreasing weight based on the given distance\n",
    "    and maximum distance.\n",
    "\n",
    "    Args:\n",
    "        distance (float): The distance at which you want to calculate the weight.\n",
    "        max_distance (float): The maximum distance at which the weight becomes 0.\n",
    "\n",
    "    Returns:\n",
    "        float: The linearly decreasing weight, ranging from 1 to 0.\n",
    "    \"\"\"\n",
    "    # Ensure that distance is within the valid range [0, max_distance]\n",
    "    distance = max(0, min(distance, max_distance))\n",
    "\n",
    "    # Calculate the weight as a linear interpolation\n",
    "    weight = 1.0 - (distance / max_distance)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2132e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect meta data for stations\n",
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "column_widths = [11, 9, 10, 7, 3, 31]\n",
    "station_df = pd.read_fwf(GHCN_meta_url, widths=column_widths, header=None,\n",
    "                      names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "# Create 80 cell grid\n",
    "grid_80_df = generate_80_cell_grid()\n",
    "\n",
    "# Initialize an empty list to store station IDs and weights as dictionaries\n",
    "station_weights_within_radius = []\n",
    "\n",
    "# Maximum distance for the weight calculation (e.g., 1200.0 km)\n",
    "max_distance = 1200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90ff19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 80/80 [00:28<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Southern  Northern  Western  Eastern  Center_Latitude  Center_Longitude  \\\n",
      "0      -90.0     -64.2   -180.0   -144.0       -71.834397            -162.0   \n",
      "1      -90.0     -64.2   -144.0   -108.0       -71.834397            -126.0   \n",
      "2      -90.0     -64.2   -108.0    -72.0       -71.834397             -90.0   \n",
      "3      -90.0     -64.2    -72.0    -36.0       -71.834397             -54.0   \n",
      "4      -90.0     -64.2    -36.0      0.0       -71.834397             -18.0   \n",
      "..       ...       ...      ...      ...              ...               ...   \n",
      "75      64.2      90.0      0.0     36.0        71.834397              18.0   \n",
      "76      64.2      90.0     36.0     72.0        71.834397              54.0   \n",
      "77      64.2      90.0     72.0    108.0        71.834397              90.0   \n",
      "78      64.2      90.0    108.0    144.0        71.834397             126.0   \n",
      "79      64.2      90.0    144.0    180.0        71.834397             162.0   \n",
      "\n",
      "                                      Nearby_Stations  \n",
      "0   {'AYM00089327': 0.033180373720041234, 'AYM0008...  \n",
      "1   {'AYM00089324': 0.23006027330624246, 'AYM00089...  \n",
      "2   {'AYM00089062': 0.19952788531365517, 'AYM00089...  \n",
      "3   {'AYM00088963': 0.21163124644915876, 'AYM00089...  \n",
      "4   {'AYM00089001': 0.5100758876554738, 'AYM000890...  \n",
      "..                                                ...  \n",
      "75  {'FI000002401': 0.084962737000011, 'FI00000750...  \n",
      "76  {'FI000007501': 0.03525474404660178, 'FIE00146...  \n",
      "77  {'RSM00020069': 0.23374072346639707, 'RSM00020...  \n",
      "78  {'RSM00020289': 0.13001416824544054, 'RSM00020...  \n",
      "79  {'RSM00021358': 0.5377131220987585, 'RSM000214...  \n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Collect meta data for stations\n",
    "# GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "# column_widths = [11, 9, 10, 7, 3, 31]\n",
    "# station_df = pd.read_fwf(GHCN_meta_url, widths=column_widths, header=None,\n",
    "#                       names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "# # Create 80 cell grid\n",
    "# grid_80_df = generate_80_cell_grid()\n",
    "\n",
    "# # Initialize an empty list to store station IDs and weights as dictionaries\n",
    "# station_weights_within_radius = []\n",
    "\n",
    "# # Maximum distance for the weight calculation (e.g., 1200.0 km)\n",
    "# max_distance = 1200.0\n",
    "\n",
    "# Use tqdm to track progress\n",
    "for index, row in tqdm(grid_80_df.iterrows(), total=len(grid_80_df), desc=\"Processing\"):\n",
    "    center_lat = row['Center_Latitude']\n",
    "    center_lon = row['Center_Longitude']\n",
    "    \n",
    "    # Calculate distances for each station in station_df\n",
    "    distances = station_df.apply(lambda x: haversine_distance(center_lat, center_lon, x['Latitude'], x['Longitude']), axis=1)\n",
    "    \n",
    "    # Find station IDs within the specified radius\n",
    "    nearby_stations = station_df[distances <= max_distance]\n",
    "    \n",
    "    # Calculate weights for each nearby station\n",
    "    weights = nearby_stations.apply(lambda x: linearly_decreasing_weight(distances[x.name], max_distance), axis=1)\n",
    "    \n",
    "    # Create a dictionary of station IDs and weights\n",
    "    station_weights = dict(zip(nearby_stations['Station_ID'], weights))\n",
    "    \n",
    "    # Append the dictionary to the result list\n",
    "    station_weights_within_radius.append(station_weights)\n",
    "\n",
    "# Add the list of station IDs and weights as a new column\n",
    "grid_80_df['Nearby_Stations'] = station_weights_within_radius\n",
    "\n",
    "# Print grid_80_df with the new column\n",
    "print(grid_80_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1463377",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AYM00089327': 0.033180373720041234,\n",
       " 'AYM00089345': 0.05384328602457955,\n",
       " 'AYM00089376': 0.16452351560386513,\n",
       " 'AYM00089661': 0.2129759486689381,\n",
       " 'AYM00089662': 0.07207839859029075,\n",
       " 'AYM00089664': 0.07705257354483985,\n",
       " 'AYM00089666': 0.031079631471481517,\n",
       " 'AYM00089667': 0.07063445329876195,\n",
       " 'AYM00089768': 0.05208054854286992,\n",
       " 'AYM00089769': 0.08431462499458553,\n",
       " 'AYM00089864': 0.06190969590953144,\n",
       " 'AYM00089865': 0.15713530446043067,\n",
       " 'AYM00089868': 0.040122552139423395,\n",
       " 'AYM00089872': 0.14907879375180066,\n",
       " 'AYM00089879': 0.23312986385784518,\n",
       " 'AYW00067401': 0.2404492620133053,\n",
       " 'AYW00067402': 0.4506642628052141,\n",
       " 'AYW00067601': 0.41011860537970757,\n",
       " 'AYW00068701': 0.1636133619479868,\n",
       " 'AYW00087602': 0.03229984214024895,\n",
       " 'AYW00087701': 0.21322487211259544}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_80_df.iloc[0]['Nearby_Stations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea04c6",
   "metadata": {},
   "source": [
    "# Xarray Conversion (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = step1_output\n",
    "\n",
    "# # Transpose the DataFrame to have months as columns\n",
    "# df_copy = df.set_index(['Year', 'Name', 'Latitude', 'Longitude']).transpose()\n",
    "\n",
    "# # Create a MultiIndex with separate levels for Station_ID, Year, and Month\n",
    "# df_copy.columns = pd.MultiIndex.from_tuples(\n",
    "#     [(station_id, year, f\"Month_{month}\") for station_id in df_copy.columns.get_level_values(0) for year in df_copy.index.get_level_values(0) for month in range(1, 13)],\n",
    "#     names=['Station_ID', 'Year', 'Month']\n",
    "# )\n",
    "\n",
    "# # Drop Year column from the copied DataFrame\n",
    "# df_copy = df_copy.drop('Year', axis=0)\n",
    "\n",
    "# # Convert the copied DataFrame to an xarray Dataset\n",
    "# ds = xr.Dataset.from_dataframe(df_copy)\n",
    "\n",
    "# # Rename latitude and longitude variables (if needed)\n",
    "# ds = ds.rename({'Longitude': 'Lon', 'Latitude': 'Lat'})\n",
    "\n",
    "# # Print the resulting xarray Dataset\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d09179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
