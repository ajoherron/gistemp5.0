{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a0545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import itertools\n",
    "import math\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad280f9",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3765258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import GHCN_temp_url, GHCN_meta_url\n",
    "GHCN_temp_url = 'https://data.giss.nasa.gov/pub/gistemp/ghcnm.tavg.qcf.dat'\n",
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "\n",
    "\n",
    "def get_GHCN_data(temp_url, meta_url):\n",
    "\n",
    "    '''\n",
    "    Retrieves and formats temperature data from the Global Historical Climatology Network (GHCN) dataset.\n",
    "\n",
    "    Args:\n",
    "    temp_url (str): The URL to the temperature data file in GHCN format.\n",
    "    meta_url (str): The URL to the metadata file containing station information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing temperature data with station metadata.\n",
    "    \n",
    "    This function sends an HTTP GET request to the temperature data URL, processes the data to create\n",
    "    a formatted DataFrame, replaces missing values with NaN, converts temperature values to degrees Celsius,\n",
    "    and merges the data with station metadata based on station IDs. The resulting DataFrame includes\n",
    "    columns for station latitude, longitude, and name, and is indexed by station IDs.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(temp_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Get the content of the response\n",
    "            file_data = response.content.decode(\"utf-8\")\n",
    "\n",
    "            # Create a list to store formatted data\n",
    "            formatted_data = []\n",
    "\n",
    "            # Loop through file data\n",
    "            for line in file_data.split('\\n'):\n",
    "                \n",
    "                # Check if line is not empty\n",
    "                if line.strip():\n",
    "                    \n",
    "                    # Extract relevant data\n",
    "                    # (Using code from GHCNV4Reader())\n",
    "                    station_id = line[:11]\n",
    "                    year = int(line[11:15])\n",
    "                    values = [int(line[i:i+5]) for i in range(19, 115, 8)]\n",
    "                    \n",
    "                    # Append data to list\n",
    "                    formatted_data.append([station_id, year] + values)\n",
    "\n",
    "            # Create DataFrame from formatted data\n",
    "            column_names = ['Station_ID', 'Year'] + [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN = pd.DataFrame(formatted_data, columns=column_names)\n",
    "            \n",
    "            # Replace -9999 with NaN\n",
    "            df_GHCN.replace(-9999, np.nan, inplace=True)\n",
    "            \n",
    "            # Format data - convert to degrees C\n",
    "            month_columns = [f'Month_{i}' for i in range(1, 13)]\n",
    "            df_GHCN[month_columns] = df_GHCN[month_columns].divide(100)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "    # Define the column widths, create meta data dataframe\n",
    "    column_widths = [11, 9, 10, 7, 3, 31]\n",
    "    df_meta = pd.read_fwf(meta_url, widths=column_widths, header=None,\n",
    "                          names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "    # Merge on station ID, set index\n",
    "    df = pd.merge(df_GHCN, df_meta[['Station_ID', 'Latitude', 'Longitude', 'Name']], on='Station_ID', how='left')\n",
    "    df = df.set_index('Station_ID')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def step0():\n",
    "    '''\n",
    "    Performs the initial data processing steps for the GHCN temperature dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing filtered and formatted temperature data.\n",
    "    \n",
    "    This function retrieves temperature data from the Global Historical Climatology Network (GHCN) dataset,\n",
    "    processes and formats the data, and returns a DataFrame. The data is first fetched using specified URLs,\n",
    "    and is returned for further analysis.\n",
    "    '''\n",
    "    df_GHCN = get_GHCN_data(GHCN_temp_url, GHCN_meta_url)\n",
    "    return df_GHCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6185064",
   "metadata": {},
   "outputs": [],
   "source": [
    "step0_output = step0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04357284",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c6a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import drop_rules\n",
    "drop_rules = '''\n",
    "CHM00052836  omit: 0-1948\n",
    "CHXLT909860  omit: 0-1950\n",
    "BL000085365  omit: 0-1930\n",
    "MXXLT948335  omit: 0-1952\n",
    "ASN00058012  omit: 0-1899\n",
    "ASN00084016  omit: 0-1899\n",
    "ASN00069018  omit: 0-1898\n",
    "NIXLT013080  omit: 0-1930\n",
    "NIXLT751359  omit: 0-9999\n",
    "CHXLT063941  omit: 0-1937\n",
    "CHM00054843  omit: 0-1937\n",
    "MXM00076373  omit: 0-9999\n",
    "USC00044022  omit: 0-9999\n",
    "USC00044025  omit: 0-9999\n",
    "CA002402332  omit: 2011-9999\n",
    "RSM00024266  omit: 2021/09\n",
    "'''\n",
    "\n",
    "\n",
    "def filter_coordinates(df):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on latitude and longitude conditions.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame with rows where latitude is between -90 and 90,\n",
    "    and longitude is between -180 and 180.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define latitude and longitude range conditions\n",
    "    lat_condition = (df['Latitude'] >= -90) & (df['Latitude'] <= 90)\n",
    "    lon_condition = (df['Longitude'] >= -180) & (df['Longitude'] <= 180)\n",
    "\n",
    "    # Apply the conditions to filter the DataFrame\n",
    "    df_filtered = df[lat_condition & lon_condition]\n",
    "    \n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(df) - len(df_filtered)\n",
    "    print(f'Number of rows with invalid coordinates (removed): {num_filtered}')\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_stations_by_rules(dataframe, rules_text):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame of climate station data based on exclusion rules specified in a text format.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing climate station data.\n",
    "        rules_text (str): A string containing exclusion rules for specific stations and years.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with stations omitted based on the provided rules.\n",
    "\n",
    "    Rules Format:\n",
    "        The 'rules_text' should be formatted as follows:\n",
    "        - Each rule is represented as a single line in the text.\n",
    "        - Each line should start with the station ID followed by exclusion rules.\n",
    "        - Exclusion rules consist of 'omit:' followed by the years to exclude, e.g., 'omit: 2000-2010'.\n",
    "        - Years can be specified as a single year (e.g., 'omit: 2000') or as a range (e.g., 'omit: 2000-2010').\n",
    "        - Year ranges can also be specified using '/' (e.g., 'omit: 2000/2002').\n",
    "\n",
    "    Example:\n",
    "        rules_text = '''\n",
    "            CHM00052836  omit: 0-1948\n",
    "            CHXLT909860  omit: 0-1950\n",
    "            BL000085365  omit: 0-1930\n",
    "            ...\n",
    "        '''\n",
    "\n",
    "    This function takes the provided rules and applies them to the input DataFrame,\n",
    "    resulting in a new DataFrame with stations excluded based on the specified rules.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the rules from the provided text\n",
    "    rules = {}\n",
    "    for line in rules_text.split('\\n'):\n",
    "        if line.strip():\n",
    "            match = re.match(r'([A-Z0-9]+)\\s+omit:\\s+(\\S+)', line)\n",
    "            if match:\n",
    "                station_id, year_rule = match.groups()\n",
    "                rules[station_id] = year_rule\n",
    "\n",
    "    # Create a mask to identify rows to omit\n",
    "    mask = pd.Series(True, index=dataframe.index)\n",
    "\n",
    "    for station_id, year_rule in rules.items():\n",
    "        try:\n",
    "            # Split the year_rule into start and end years\n",
    "            start_year, end_year = map(int, year_rule.split('-'))\n",
    "        except ValueError:\n",
    "            # Handle cases like '2011/12' or '2012-9999'\n",
    "            if '/' in year_rule:\n",
    "                start_year = int(year_rule.split('/')[0])\n",
    "                end_year = start_year\n",
    "            elif '-' in year_rule:\n",
    "                start_year = int(year_rule.split('-')[0])\n",
    "                end_year = int(year_rule.split('-')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Update the mask to False for the specified range of years for the station_id\n",
    "        mask &= ~((dataframe['Year'] >= start_year) & (dataframe['Year'] <= end_year) & (dataframe.index == station_id))\n",
    "\n",
    "    # Apply the mask to filter the DataFrame\n",
    "    filtered_dataframe = dataframe[mask]\n",
    "\n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(dataframe) - len(filtered_dataframe)\n",
    "    print(f'Number of rows removed according to station exclusion rules: {num_filtered}')\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "\n",
    "def step1(step0_output):\n",
    "    \"\"\"\n",
    "    Applies data filtering and cleaning operations to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        step0_output (pd.DataFrame): The initial DataFrame containing climate station data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame ready for further analysis.\n",
    "\n",
    "    This function serves as a data processing step by applying two essential filtering operations:\n",
    "    1. `filter_coordinates`: Filters the DataFrame based on geographical coordinates, retaining relevant stations.\n",
    "    2. `filter_stations_by_rules`: Filters the DataFrame based on exclusion rules, omitting specified stations and years.\n",
    "\n",
    "    The resulting DataFrame is cleaned of irrelevant stations and years according to specified rules\n",
    "    and is ready for subsequent data analysis or visualization.\n",
    "    \"\"\"\n",
    "        \n",
    "    df_filtered = filter_coordinates(step0_output)\n",
    "    df_clean = filter_stations_by_rules(df_filtered, drop_rules)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb47b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with invalid coordinates (removed): 194947\n",
      "Number of rows removed according to station exclusion rules: 344\n"
     ]
    }
   ],
   "source": [
    "step1_output = step1(step0_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a5434",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d8704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d38270",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c881e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_80_cell_grid():\n",
    "    n_bands = 8  # Number of latitude bands\n",
    "    n_boxes_per_band = 10  # Number of boxes per band\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for band in range(n_bands):\n",
    "        lat_south = -90 + band * (180 / n_bands)\n",
    "        lat_north = -90 + (band + 1) * (180 / n_bands)\n",
    "\n",
    "        for i in range(n_boxes_per_band):\n",
    "            lon_west = -180 + i * (360 / n_boxes_per_band)\n",
    "            lon_east = -180 + (i + 1) * (360 / n_boxes_per_band)\n",
    "\n",
    "            data.append((lat_south, lat_north, lon_west, lon_east))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "    \n",
    "\n",
    "    # Calculate the center latitude and longitude\n",
    "    df['Center_Latitude'] = (df['Southern'] + df['Northern']) / 2\n",
    "    df['Center_Longitude'] = (df['Western'] + df['Eastern']) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lerp(x, y, p):\n",
    "    return y * p + (1 - p) * x\n",
    "\n",
    "def generate_8000_cell_grid():\n",
    "    def subgen(lat_s, lat_n, lon_w, lon_e):\n",
    "        alts = math.sin(lat_s * math.pi / 180)\n",
    "        altn = math.sin(lat_n * math.pi / 180)\n",
    "        for y in range(10):\n",
    "            s = 180 * math.asin(lerp(alts, altn, y * 0.1)) / math.pi\n",
    "            n = 180 * math.asin(lerp(alts, altn, (y + 1) * 0.1)) / math.pi\n",
    "            for x in range(10):\n",
    "                w = lerp(lon_w, lon_e, x * 0.1)\n",
    "                e = lerp(lon_w, lon_e, (x + 1) * 0.1)\n",
    "                yield (s, n, w, e)\n",
    "\n",
    "    initial_regions_df = generate_80_cell_grid()\n",
    "    data = []\n",
    "\n",
    "    for index, row in initial_regions_df.iterrows():\n",
    "        for subcell in subgen(row['Southern'], row['Northern'], row['Western'], row['Eastern']):\n",
    "            data.append(subcell)\n",
    "\n",
    "    grid_df = pd.DataFrame(data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "    \n",
    "    # Calculate the center latitude and longitude\n",
    "    grid_df['Center_Latitude'] = (grid_df['Southern'] + grid_df['Northern']) / 2\n",
    "    grid_df['Center_Longitude'] = (grid_df['Western'] + grid_df['Eastern']) / 2\n",
    "    \n",
    "    return grid_df\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the spherical distance (in kilometers) between two pairs of\n",
    "    latitude and longitude coordinates using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Spherical distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    radius = 6371.0  # Earth's mean radius\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def linearly_decreasing_weight(distance, max_distance):\n",
    "    \"\"\"\n",
    "    Calculate a linearly decreasing weight based on the given distance\n",
    "    and maximum distance.\n",
    "\n",
    "    Args:\n",
    "        distance (float): The distance at which you want to calculate the weight.\n",
    "        max_distance (float): The maximum distance at which the weight becomes 0.\n",
    "\n",
    "    Returns:\n",
    "        float: The linearly decreasing weight, ranging from 1 to 0.\n",
    "    \"\"\"\n",
    "    # Ensure that distance is within the valid range [0, max_distance]\n",
    "    distance = max(0, min(distance, max_distance))\n",
    "\n",
    "    # Calculate the weight as a linear interpolation\n",
    "    weight = 1.0 - (distance / max_distance)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5bbb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "GHCN_meta_url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "column_widths = [11, 9, 10, 7, 3, 31]\n",
    "df_meta = pd.read_fwf(GHCN_meta_url, widths=column_widths, header=None,\n",
    "                      names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "grid_8000_df = generate_8000_cell_grid()\n",
    "grid_80_df = generate_80_cell_grid()\n",
    "station_df = df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e5c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 80/80 [00:28<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Southern  Northern  Western  Eastern  Center_Latitude  Center_Longitude  \\\n",
      "0      -90.0     -67.5   -180.0   -144.0           -78.75            -162.0   \n",
      "1      -90.0     -67.5   -144.0   -108.0           -78.75            -126.0   \n",
      "2      -90.0     -67.5   -108.0    -72.0           -78.75             -90.0   \n",
      "3      -90.0     -67.5    -72.0    -36.0           -78.75             -54.0   \n",
      "4      -90.0     -67.5    -36.0      0.0           -78.75             -18.0   \n",
      "..       ...       ...      ...      ...              ...               ...   \n",
      "75      67.5      90.0      0.0     36.0            78.75              18.0   \n",
      "76      67.5      90.0     36.0     72.0            78.75              54.0   \n",
      "77      67.5      90.0     72.0    108.0            78.75              90.0   \n",
      "78      67.5      90.0    108.0    144.0            78.75             126.0   \n",
      "79      67.5      90.0    144.0    180.0            78.75             162.0   \n",
      "\n",
      "                                      Nearby_Stations  \n",
      "0   [17479, 17480, 17481, 17482, 17483, 17484, 174...  \n",
      "1   [17471, 17479, 17480, 17481, 17482, 17483, 174...  \n",
      "2   [17469, 17471, 17474, 17476, 17478, 17479, 174...  \n",
      "3   [17454, 17456, 17457, 17469, 17471, 17472, 174...  \n",
      "4   [17450, 17451, 17452, 17453, 17454, 17455, 174...  \n",
      "..                                                ...  \n",
      "75  [34322, 34325, 34326, 34329, 34338, 34340, 343...  \n",
      "76  [46884, 46885, 46893, 46897, 46902, 46903, 469...  \n",
      "77  [47858, 47859, 47860, 47861, 47862, 47863, 478...  \n",
      "78  [47860, 47861, 47862, 47863, 47864, 47867, 478...  \n",
      "79  [47876, 47877, 47880, 47881, 47886, 47887, 478...  \n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store station IDs\n",
    "station_ids_within_radius = []\n",
    "\n",
    "# Use tqdm to track progress\n",
    "for index, row in tqdm(grid_80_df.iterrows(), total=len(grid_80_df), desc=\"Processing\"):\n",
    "    center_lat = row['Center_Latitude']\n",
    "    center_lon = row['Center_Longitude']\n",
    "    \n",
    "    # Calculate distances for each station in station_df\n",
    "    distances = station_df.apply(lambda x: haversine_distance(center_lat, center_lon, x['Latitude'], x['Longitude']), axis=1)\n",
    "    \n",
    "    # Find station IDs within 1200 km radius\n",
    "    nearby_stations = station_df[distances <= 1200.0].index.tolist()\n",
    "    \n",
    "    # Append the list of nearby station IDs to the result list\n",
    "    station_ids_within_radius.append(nearby_stations)\n",
    "\n",
    "# Add the list of station IDs to grid_80_df as a new column\n",
    "grid_80_df['Nearby_Stations'] = station_ids_within_radius\n",
    "\n",
    "# Print grid_80_df with the new column\n",
    "print(grid_80_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cac52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17479,\n",
       " 17480,\n",
       " 17481,\n",
       " 17482,\n",
       " 17483,\n",
       " 17484,\n",
       " 17485,\n",
       " 17486,\n",
       " 17511,\n",
       " 17512,\n",
       " 17513,\n",
       " 17514,\n",
       " 17515,\n",
       " 17516,\n",
       " 17520,\n",
       " 17521,\n",
       " 17528,\n",
       " 17529,\n",
       " 17530,\n",
       " 17531,\n",
       " 17532,\n",
       " 17533,\n",
       " 17534,\n",
       " 17538,\n",
       " 17539,\n",
       " 17540,\n",
       " 17541,\n",
       " 17542,\n",
       " 17545,\n",
       " 17546,\n",
       " 17547,\n",
       " 17548]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_80_df.iloc[0]['Nearby_Stations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be48bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a9f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c8d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383634a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49716f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46fbceb",
   "metadata": {},
   "source": [
    "# Xarray Conversion (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5af5df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = step1_output\n",
    "\n",
    "# # Transpose the DataFrame to have months as columns\n",
    "# df_copy = df.set_index(['Year', 'Name', 'Latitude', 'Longitude']).transpose()\n",
    "\n",
    "# # Create a MultiIndex with separate levels for Station_ID, Year, and Month\n",
    "# df_copy.columns = pd.MultiIndex.from_tuples(\n",
    "#     [(station_id, year, f\"Month_{month}\") for station_id in df_copy.columns.get_level_values(0) for year in df_copy.index.get_level_values(0) for month in range(1, 13)],\n",
    "#     names=['Station_ID', 'Year', 'Month']\n",
    "# )\n",
    "\n",
    "# # Drop Year column from the copied DataFrame\n",
    "# df_copy = df_copy.drop('Year', axis=0)\n",
    "\n",
    "# # Convert the copied DataFrame to an xarray Dataset\n",
    "# ds = xr.Dataset.from_dataframe(df_copy)\n",
    "\n",
    "# # Rename latitude and longitude variables (if needed)\n",
    "# ds = ds.rename({'Longitude': 'Lon', 'Latitude': 'Lat'})\n",
    "\n",
    "# # Print the resulting xarray Dataset\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea161717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
