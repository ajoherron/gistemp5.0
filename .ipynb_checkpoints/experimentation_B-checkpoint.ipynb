{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a0545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import itertools\n",
    "import math\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013b1a4",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cb9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 0: Downloading Data\n",
    "\n",
    "Combining diverse inputs into a single dataset\n",
    "\n",
    "Inputs include:\n",
    "    - GHCN v4 data\n",
    "    - ERRST v5 data (later on?)\n",
    "'''\n",
    "\n",
    "# Standard library imports\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# 3rd-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import GHCN_temp_url, GHCN_meta_url\n",
    "GHCN_temp_url = 'https://data.giss.nasa.gov/pub/gistemp/ghcnm.tavg.qcf.dat'\n",
    "GHCN_meta_url = 'https://data.giss.nasa.gov/pub/gistemp/v4.inv'\n",
    "\n",
    "# Local imports\n",
    "from parameters.data import GHCN_temp_url, GHCN_meta_url\n",
    "\n",
    "def get_GHCN_data(temp_url: str, meta_url: str, start_year: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Retrieves and formats temperature data from the Global Historical Climatology Network (GHCN) dataset.\n",
    "\n",
    "    Args:\n",
    "    temp_url (str): The URL to the temperature data file in GHCN format.\n",
    "    meta_url (str): The URL to the metadata file containing station information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing temperature data with station metadata.\n",
    "    \n",
    "    This function sends an HTTP GET request to the temperature data URL, processes the data to create\n",
    "    a formatted DataFrame, replaces missing values with NaN, converts temperature values to degrees Celsius,\n",
    "    and merges the data with station metadata based on station IDs. The resulting DataFrame includes\n",
    "    columns for station latitude, longitude, and name, and is indexed by station IDs.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(temp_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Get the content of the response\n",
    "            file_data: str = response.content.decode(\"utf-8\")\n",
    "\n",
    "            # Create a list to store formatted data\n",
    "            formatted_data = []\n",
    "\n",
    "            # Loop through file data\n",
    "            for line in file_data.split('\\n'):\n",
    "                \n",
    "                # Check if line is not empty\n",
    "                if line.strip():\n",
    "                    \n",
    "                    # Extract relevant data\n",
    "                    # (Using code from GHCNV4Reader())\n",
    "                    station_id: str = line[:11]\n",
    "                    year: int = int(line[11:15])\n",
    "                    values: List[int] = [int(line[i:i+5]) for i in range(19, 115, 8)]\n",
    "                    \n",
    "                    # Append data to list\n",
    "                    formatted_data.append([station_id, year] + values)\n",
    "\n",
    "            # Create DataFrame from formatted data\n",
    "            column_names: List[str] = ['Station_ID', 'Year'] + [f'{i}' for i in range(1, 13)]\n",
    "            df_GHCN: pd.DataFrame = pd.DataFrame(formatted_data, columns=column_names)\n",
    "            \n",
    "            # Replace -9999 with NaN\n",
    "            df_GHCN.replace(-9999, np.nan, inplace=True)\n",
    "            \n",
    "            # Format data - convert to degrees C\n",
    "            month_columns: List[str] = [f'{i}' for i in range(1, 13)]\n",
    "            df_GHCN[month_columns] = df_GHCN[month_columns].divide(100)\n",
    "            \n",
    "            # Drop all years before start year\n",
    "            start_year_mask = df_GHCN['Year'] >= 1850\n",
    "            df_GHCN = df_GHCN.loc[start_year_mask]\n",
    "            \n",
    "        else:\n",
    "            print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        \n",
    "    # Pivot the dataframe\n",
    "    pivoted_df = df_GHCN.pivot(index='Station_ID', columns='Year')\n",
    "\n",
    "    # Flatten the multi-level columns and format them as desired\n",
    "    pivoted_df.columns = [f\"{col[0]}_{col[1]}\" for col in pivoted_df.columns]\n",
    "\n",
    "    # Sort the columns by the month number\n",
    "    sorted_columns = sorted(pivoted_df.columns, key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "    # Reorder the dataframe columns\n",
    "    pivoted_df = pivoted_df[sorted_columns]\n",
    "\n",
    "    # Reset the index\n",
    "    pivoted_df.reset_index(inplace=True)\n",
    "\n",
    "    # Define the column widths, create meta data dataframe\n",
    "    column_widths: List[int] = [11, 9, 10, 7, 3, 31]\n",
    "    df_meta: pd.DataFrame = pd.read_fwf(meta_url, widths=column_widths, header=None,\n",
    "                          names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])\n",
    "\n",
    "    # Merge on station ID, set index, drop station names\n",
    "    df: pd.DataFrame = pd.merge(pivoted_df, df_meta[['Station_ID', 'Latitude', 'Longitude', 'Name']], on='Station_ID', how='left')\n",
    "    df.set_index('Station_ID', inplace=True)\n",
    "    df.drop(columns='Name', inplace=True) \n",
    "\n",
    "    return df\n",
    "\n",
    "def step0() -> pd.DataFrame:\n",
    "    '''\n",
    "    Performs the initial data processing steps for the GHCN temperature dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame containing filtered and formatted temperature data.\n",
    "    \n",
    "    This function retrieves temperature data from the Global Historical Climatology Network (GHCN) dataset,\n",
    "    processes and formats the data, and returns a DataFrame. The data is first fetched using specified URLs,\n",
    "    and is returned for further analysis.\n",
    "    '''\n",
    "    df_GHCN: pd.DataFrame = get_GHCN_data(GHCN_temp_url, GHCN_meta_url, 1850)\n",
    "    return df_GHCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d8bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step0_output = step0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4bf201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1850</th>\n",
       "      <th>2_1850</th>\n",
       "      <th>3_1850</th>\n",
       "      <th>4_1850</th>\n",
       "      <th>5_1850</th>\n",
       "      <th>6_1850</th>\n",
       "      <th>7_1850</th>\n",
       "      <th>8_1850</th>\n",
       "      <th>9_1850</th>\n",
       "      <th>10_1850</th>\n",
       "      <th>...</th>\n",
       "      <th>5_2023</th>\n",
       "      <th>6_2023</th>\n",
       "      <th>7_2023</th>\n",
       "      <th>8_2023</th>\n",
       "      <th>9_2023</th>\n",
       "      <th>10_2023</th>\n",
       "      <th>11_2023</th>\n",
       "      <th>12_2023</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Station_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACW00011604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.7667</td>\n",
       "      <td>11.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AE000041196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.89</td>\n",
       "      <td>34.68</td>\n",
       "      <td>36.65</td>\n",
       "      <td>36.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041184</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.6170</td>\n",
       "      <td>55.9330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.70</td>\n",
       "      <td>35.18</td>\n",
       "      <td>37.38</td>\n",
       "      <td>37.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.4300</td>\n",
       "      <td>54.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZI000067983</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZI000067991</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT371333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.8300</td>\n",
       "      <td>31.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT443557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.9800</td>\n",
       "      <td>32.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT622116</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19.4300</td>\n",
       "      <td>29.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27676 rows × 2090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1_1850  2_1850  3_1850  4_1850  5_1850  6_1850  7_1850  8_1850  \\\n",
       "Station_ID                                                                    \n",
       "ACW00011604     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AE000041196     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041184     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041194     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041216     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "ZI000067983     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZI000067991     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT371333     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT443557     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT622116     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "             9_1850  10_1850  ...  5_2023  6_2023  7_2023  8_2023  9_2023  \\\n",
       "Station_ID                    ...                                           \n",
       "ACW00011604     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "AE000041196     NaN      NaN  ...   31.89   34.68   36.65   36.67     NaN   \n",
       "AEM00041184     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041194     NaN      NaN  ...   32.70   35.18   37.38   37.79     NaN   \n",
       "AEM00041216     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "...             ...      ...  ...     ...     ...     ...     ...     ...   \n",
       "ZI000067983     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZI000067991     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT371333     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT443557     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT622116     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "             10_2023  11_2023  12_2023  Latitude  Longitude  \n",
       "Station_ID                                                   \n",
       "ACW00011604      NaN      NaN      NaN   57.7667    11.8667  \n",
       "AE000041196      NaN      NaN      NaN   25.3330    55.5170  \n",
       "AEM00041184      NaN      NaN      NaN   25.6170    55.9330  \n",
       "AEM00041194      NaN      NaN      NaN   25.2550    55.3640  \n",
       "AEM00041216      NaN      NaN      NaN   24.4300    54.4700  \n",
       "...              ...      ...      ...       ...        ...  \n",
       "ZI000067983      NaN      NaN      NaN  -20.2000    32.6160  \n",
       "ZI000067991      NaN      NaN      NaN  -22.2170    30.0000  \n",
       "ZIXLT371333      NaN      NaN      NaN  -17.8300    31.0200  \n",
       "ZIXLT443557      NaN      NaN      NaN  -18.9800    32.4500  \n",
       "ZIXLT622116      NaN      NaN      NaN  -19.4300    29.7500  \n",
       "\n",
       "[27676 rows x 2090 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step0_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad6e23",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31228a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Removal of bad data\n",
    "\n",
    "Drop or adjust certain records (or parts of records).\n",
    "This includes outliers / out of range reports.\n",
    "Determined using configuration file.\n",
    "    <TO-DO> Figure out if this method is ideal.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Add the parent folder to sys.path\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Local imports\n",
    "# from parameters.data import drop_rules\n",
    "drop_rules = '''\n",
    "CHM00052836 omit: 0-1948\n",
    "CHXLT909860 omit: 0-1950\n",
    "BL000085365 omit: 0-1930\n",
    "MXXLT948335 omit: 0-1952\n",
    "ASN00058012 omit: 0-1899\n",
    "ASN00084016 omit: 0-1899\n",
    "ASN00069018 omit: 0-1898\n",
    "NIXLT013080 omit: 0-1930\n",
    "NIXLT751359 omit: 0-9999\n",
    "CHXLT063941 omit: 0-1937\n",
    "CHM00054843 omit: 0-1937\n",
    "MXM00076373 omit: 0-9999\n",
    "USC00044022 omit: 0-9999\n",
    "USC00044025 omit: 0-9999\n",
    "CA002402332 omit: 2011-9999\n",
    "RSM00024266 omit: 2021/09\n",
    "'''\n",
    "\n",
    "\n",
    "def filter_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on latitude and longitude conditions.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame with rows where latitude is between -90 and 90,\n",
    "    and longitude is between -180 and 180.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define latitude and longitude range conditions\n",
    "    lat_condition = (df['Latitude'] >= -90) & (df['Latitude'] <= 90)\n",
    "    lon_condition = (df['Longitude'] >= -180) & (df['Longitude'] <= 180)\n",
    "\n",
    "    # Apply the conditions using the .loc indexer\n",
    "    df_filtered = df.loc[lat_condition & lon_condition]\n",
    "    \n",
    "    # Calculate number of rows filtered\n",
    "    num_filtered = len(df) - len(df_filtered)\n",
    "    print(f'Number of stations with invalid coordinates (removed): {num_filtered}')\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "044e6ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations with invalid coordinates (removed): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1850</th>\n",
       "      <th>2_1850</th>\n",
       "      <th>3_1850</th>\n",
       "      <th>4_1850</th>\n",
       "      <th>5_1850</th>\n",
       "      <th>6_1850</th>\n",
       "      <th>7_1850</th>\n",
       "      <th>8_1850</th>\n",
       "      <th>9_1850</th>\n",
       "      <th>10_1850</th>\n",
       "      <th>...</th>\n",
       "      <th>5_2023</th>\n",
       "      <th>6_2023</th>\n",
       "      <th>7_2023</th>\n",
       "      <th>8_2023</th>\n",
       "      <th>9_2023</th>\n",
       "      <th>10_2023</th>\n",
       "      <th>11_2023</th>\n",
       "      <th>12_2023</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Station_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACW00011604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.7667</td>\n",
       "      <td>11.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AE000041196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.89</td>\n",
       "      <td>34.68</td>\n",
       "      <td>36.65</td>\n",
       "      <td>36.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041184</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.6170</td>\n",
       "      <td>55.9330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.70</td>\n",
       "      <td>35.18</td>\n",
       "      <td>37.38</td>\n",
       "      <td>37.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AEM00041216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.4300</td>\n",
       "      <td>54.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZI000067983</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-20.2000</td>\n",
       "      <td>32.6160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZI000067991</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.2170</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT371333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.8300</td>\n",
       "      <td>31.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT443557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.9800</td>\n",
       "      <td>32.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXLT622116</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19.4300</td>\n",
       "      <td>29.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27676 rows × 2090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1_1850  2_1850  3_1850  4_1850  5_1850  6_1850  7_1850  8_1850  \\\n",
       "Station_ID                                                                    \n",
       "ACW00011604     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AE000041196     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041184     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041194     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041216     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "ZI000067983     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZI000067991     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT371333     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT443557     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT622116     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "             9_1850  10_1850  ...  5_2023  6_2023  7_2023  8_2023  9_2023  \\\n",
       "Station_ID                    ...                                           \n",
       "ACW00011604     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "AE000041196     NaN      NaN  ...   31.89   34.68   36.65   36.67     NaN   \n",
       "AEM00041184     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "AEM00041194     NaN      NaN  ...   32.70   35.18   37.38   37.79     NaN   \n",
       "AEM00041216     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "...             ...      ...  ...     ...     ...     ...     ...     ...   \n",
       "ZI000067983     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZI000067991     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT371333     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT443557     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "ZIXLT622116     NaN      NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "             10_2023  11_2023  12_2023  Latitude  Longitude  \n",
       "Station_ID                                                   \n",
       "ACW00011604      NaN      NaN      NaN   57.7667    11.8667  \n",
       "AE000041196      NaN      NaN      NaN   25.3330    55.5170  \n",
       "AEM00041184      NaN      NaN      NaN   25.6170    55.9330  \n",
       "AEM00041194      NaN      NaN      NaN   25.2550    55.3640  \n",
       "AEM00041216      NaN      NaN      NaN   24.4300    54.4700  \n",
       "...              ...      ...      ...       ...        ...  \n",
       "ZI000067983      NaN      NaN      NaN  -20.2000    32.6160  \n",
       "ZI000067991      NaN      NaN      NaN  -22.2170    30.0000  \n",
       "ZIXLT371333      NaN      NaN      NaN  -17.8300    31.0200  \n",
       "ZIXLT443557      NaN      NaN      NaN  -18.9800    32.4500  \n",
       "ZIXLT622116      NaN      NaN      NaN  -19.4300    29.7500  \n",
       "\n",
       "[27676 rows x 2090 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filter_coordinates(step0_output)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72edc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71400681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def filter_dataframe_by_rules(dataframe, drop_rules):\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    filtered_dataframe = dataframe.copy()\n",
    "\n",
    "    # Split the drop_rules by newline to process each rule\n",
    "    rule_lines = drop_rules.strip().split('\\n')\n",
    "\n",
    "    for rule_line in rule_lines:\n",
    "        # Split each rule line into station_id and years\n",
    "        station_id, years_to_drop = rule_line.strip().split(' omit: ')\n",
    "\n",
    "        # Split years into individual ranges\n",
    "        year_ranges = re.findall(r'(\\d+-\\d+|\\d+)', years_to_drop)\n",
    "\n",
    "        for year_range in year_ranges:\n",
    "            # Split each year range into start and end years\n",
    "            years = year_range.split('-')\n",
    "            start_year = int(years[0])\n",
    "            end_year = int(years[-1])\n",
    "\n",
    "            # Create a list of column names to update\n",
    "            columns_to_update = [f\"{year % 100}_{year}\" for year in range(start_year, end_year + 1)]\n",
    "\n",
    "            # Set all the selected columns for the station to NaN\n",
    "            filtered_dataframe.loc[station_id, columns_to_update] = np.nan\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "# Example usage:\n",
    "# df is your original DataFrame\n",
    "# drop_rules is the rules text you provided\n",
    "# filtered_df will contain NaN values based on the rules\n",
    "filtered_df = filter_dataframe_by_rules(df, drop_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba4bde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def filter_dataframe_by_rules(dataframe, drop_rules):\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    filtered_dataframe = dataframe.copy()\n",
    "\n",
    "    # Split the drop_rules by newline to process each rule\n",
    "    rule_lines = drop_rules.strip().split('\\n')\n",
    "\n",
    "    for rule_line in rule_lines:\n",
    "        # Split each rule line into station_id and years\n",
    "        station_id, years_to_drop = rule_line.strip().split(' omit: ')\n",
    "\n",
    "        # Split years into individual ranges\n",
    "        year_ranges = re.findall(r'(\\d+-\\d+|\\d+)', years_to_drop)\n",
    "\n",
    "        for year_range in year_ranges:\n",
    "            # Split each year range into start and end years\n",
    "            years = year_range.split('-')\n",
    "            start_year = int(years[0])\n",
    "            end_year = int(years[-1])\n",
    "\n",
    "            # Iterate over the range of years and set values to NaN\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                year_str = str(year)\n",
    "                if year_str in filtered_dataframe.columns:\n",
    "                    filtered_dataframe.at[station_id, year_str] = np.nan\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "# Example usage:\n",
    "# df is your original DataFrame\n",
    "# drop_rules is the rules text you provided\n",
    "# filtered_df will contain NaN values based on the rules\n",
    "filtered_df = filter_dataframe_by_rules(df, drop_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a46f3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14579fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd218ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_stations_by_rules(dataframe: pd.DataFrame, rules_text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters a DataFrame of climate station data based on exclusion rules specified in a text format.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing climate station data.\n",
    "        rules_text (str): A string containing exclusion rules for specific stations and years.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with NaN values assigned to cells based on the provided rules.\n",
    "\n",
    "    Rules Format:\n",
    "        The 'rules_text' should be formatted as follows:\n",
    "        - Each rule is represented as a single line in the text.\n",
    "        - Each line should start with the station ID followed by exclusion rules.\n",
    "        - Exclusion rules consist of 'omit:' followed by the years to exclude, e.g., 'omit: 2000-2010'.\n",
    "        - Years can be specified as a single year (e.g., 'omit: 2000') or as a range (e.g., 'omit: 2000-2010').\n",
    "        - Year ranges can also be specified using '/' (e.g., 'omit: 2000/2002').\n",
    "\n",
    "    Example:\n",
    "        rules_text = '''\n",
    "            CHM00052836  omit: 0-1948\n",
    "            CHXLT909860  omit: 0-1950\n",
    "            BL000085365  omit: 0-1930\n",
    "            ...\n",
    "        '''\n",
    "\n",
    "    This function takes the provided rules and applies them to the input DataFrame,\n",
    "    resulting in a new DataFrame with NaN values assigned to cells based on the specified rules.\n",
    "    \"\"\"\n",
    "    # Parse the rules from the provided text\n",
    "    rules = {}\n",
    "    for line in rules_text.split('\\n'):\n",
    "        if line.strip():\n",
    "            match = re.match(r'([A-Z0-9]+)\\s+omit:\\s+(\\S+)', line)\n",
    "            if match:\n",
    "                station_id, year_rule = match.groups()\n",
    "                rules[station_id] = year_rule\n",
    "\n",
    "    # Create a copy of the input DataFrame to modify\n",
    "    filtered_dataframe = dataframe.copy()\n",
    "\n",
    "    cells_changed_to_nan = 0\n",
    "\n",
    "    for station_id, year_rule in rules.items():\n",
    "        try:\n",
    "            # Split the year_rule into start and end years\n",
    "            start_year, end_year = map(int, year_rule.split('-'))\n",
    "        except ValueError:\n",
    "            # Handle cases like '2011/12' or '2012-9999'\n",
    "            if '/' in year_rule:\n",
    "                start_year = int(year_rule.split('/')[0])\n",
    "                end_year = start_year\n",
    "            elif '-' in year_rule:\n",
    "                start_year = int(year_rule.split('-')[0])\n",
    "                end_year = int(year_rule.split('-')[1])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Identify columns within the specified year range\n",
    "        columns_to_mask = [col for col in filtered_dataframe.columns if start_year <= int(col.split('_')[1]) <= end_year]\n",
    "\n",
    "        # Assign NaN values to the identified cells for the station_id\n",
    "        filtered_dataframe.loc[station_id, columns_to_mask] = np.nan\n",
    "\n",
    "        # Update the count of cells changed to NaN\n",
    "        cells_changed_to_nan += len(columns_to_mask)\n",
    "\n",
    "    print(f'Number of months changed to NaN according to station exclusion rules: {cells_changed_to_nan}')\n",
    "\n",
    "    return filtered_dataframe\n",
    "\n",
    "def step1(step0_output: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies data filtering and cleaning operations to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        step0_output (pd.DataFrame): The initial DataFrame containing climate station data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and filtered DataFrame ready for further analysis.\n",
    "\n",
    "    This function serves as a data processing step by applying two essential filtering operations:\n",
    "    1. `filter_coordinates`: Filters the DataFrame based on geographical coordinates, retaining relevant stations.\n",
    "    2. `filter_stations_by_rules`: Filters the DataFrame based on exclusion rules, omitting specified stations and years.\n",
    "\n",
    "    The resulting DataFrame is cleaned of irrelevant stations and years according to specified rules\n",
    "    and is ready for subsequent data analysis or visualization.\n",
    "    \"\"\"\n",
    "        \n",
    "    df_filtered = filter_coordinates(step0_output)\n",
    "    df_clean = filter_stations_by_rules(df_filtered, drop_rules)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b24b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations with invalid coordinates (removed): 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step1_output \u001b[38;5;241m=\u001b[39m step1(step0_output)\n",
      "Cell \u001b[0;32mIn[13], line 157\u001b[0m, in \u001b[0;36mstep1\u001b[0;34m(step0_output)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mApplies data filtering and cleaning operations to the input DataFrame.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03mand is ready for subsequent data analysis or visualization.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m filter_coordinates(step0_output)\n\u001b[0;32m--> 157\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m filter_stations_by_rules(df_filtered, drop_rules)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_clean\n",
      "Cell \u001b[0;32mIn[13], line 126\u001b[0m, in \u001b[0;36mfilter_stations_by_rules\u001b[0;34m(dataframe, rules_text)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Identify columns within the specified year range\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m columns_to_mask \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m filtered_dataframe\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m start_year \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_year]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Assign NaN values to the identified cells for the station_id\u001b[39;00m\n\u001b[1;32m    129\u001b[0m filtered_dataframe\u001b[38;5;241m.\u001b[39mloc[station_id, columns_to_mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "Cell \u001b[0;32mIn[13], line 126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Identify columns within the specified year range\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m columns_to_mask \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m filtered_dataframe\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m start_year \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_year]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Assign NaN values to the identified cells for the station_id\u001b[39;00m\n\u001b[1;32m    129\u001b[0m filtered_dataframe\u001b[38;5;241m.\u001b[39mloc[station_id, columns_to_mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "step1_output = step1(step0_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8955bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = step0_output\n",
    "df1 = df_clean #step1_output\n",
    "\n",
    "station_rules = [x for x in drop_rules.split('\\n') if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab886cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHM00052836\n",
      "0-1948\n",
      "1_1850        NaN\n",
      "2_1850        NaN\n",
      "3_1850        NaN\n",
      "4_1850        NaN\n",
      "5_1850        NaN\n",
      "             ... \n",
      "10_2023       NaN\n",
      "11_2023       NaN\n",
      "12_2023       NaN\n",
      "Latitude     36.3\n",
      "Longitude    98.1\n",
      "Name: CHM00052836, Length: 2090, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in station_rules:\n",
    "    station = i.split(' omit: ')[0]\n",
    "    print(station)\n",
    "    years = i.split(' omit: ')[1]\n",
    "    print(years)\n",
    "    \n",
    "    # Test unfiltered years\n",
    "    station = df0.loc[station]\n",
    "    print(station)\n",
    "    #columns = station.columns#.tolist()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "821ece12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHM00052836\n",
      "0-1948\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#values = [x if not np.isnan(x) else np.nan for x in values]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m valid_years \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(values)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(values[j]):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "for i in station_rules:\n",
    "    station = i.split(' omit: ')[0]\n",
    "    print(station)\n",
    "    years = i.split(' omit: ')[1]\n",
    "    print(years)\n",
    "    \n",
    "    # Test unfiltered years\n",
    "    station = df0.loc[station]\n",
    "    columns = station.index.tolist()\n",
    "    values = station.iloc[0].tolist()\n",
    "    #values = [x if not np.isnan(x) else np.nan for x in values]\n",
    "    valid_years = []\n",
    "    num = len(values)\n",
    "    for j in range(num):\n",
    "        if np.isnan(values[j]):\n",
    "            pass\n",
    "        else:\n",
    "            year = columns[j].split('_')[1]\n",
    "            valid_years.append(year)\n",
    "    valid_years = sorted(list(set(valid_years)))\n",
    "    print(valid_years)\n",
    "    \n",
    "    # Test filtered years\n",
    "    station = df1.loc[station]\n",
    "    columns = station.columns.tolist()\n",
    "    values = station.iloc[0].tolist()\n",
    "    values = [x if not np.isnan(x) else np.nan for x in values]\n",
    "    valid_years = []\n",
    "    num = len(values)\n",
    "    for j in range(num):\n",
    "        if np.isnan(values[j]):\n",
    "            pass\n",
    "        else:\n",
    "            year = columns[j].split('_')[1]\n",
    "            valid_years.append(year)\n",
    "    valid_years = sorted(list(set(valid_years)))\n",
    "    print(valid_years)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d08276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e156c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbfcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfeea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6eb948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6004502",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0e24b",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: Gridding of cells\n",
    "\n",
    "There are 8000 cells across the globe.\n",
    "Each cell's values are computed using station records within a 1200km radius.\n",
    "    - Contributions are weighted according to distance to cell center\n",
    "    (linearly decreasing to 0 at distance 1200km)\n",
    "'''\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "\n",
    "\n",
    "def calculate_area(row: Series) -> float:\n",
    "    earth_radius_km: float = 6371.0\n",
    "    delta_longitude: float = np.radians(row['Eastern'] - row['Western'])\n",
    "    southern_latitude: float = np.radians(row['Southern'])\n",
    "    northern_latitude: float = np.radians(row['Northern'])\n",
    "    area: float = (earth_radius_km ** 2) * delta_longitude * (np.sin(northern_latitude) - np.sin(southern_latitude))\n",
    "    return area\n",
    "\n",
    "\n",
    "def calculate_center_coordinates(row: pd.Series) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate the center latitude and longitude for a given box.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A Pandas Series representing a row of the DataFrame with ('southern', 'northern', 'western', 'eastern') coordinates.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing the center latitude and longitude.\n",
    "    \"\"\"\n",
    "    center_latitude = 0.5 * (math.sin(row['Southern'] * math.pi / 180) + math.sin(row['Northern'] * math.pi / 180))\n",
    "    center_longitude = 0.5 * (row['Western'] + row['Eastern'])\n",
    "    center_latitude = math.asin(center_latitude) * 180 / math.pi\n",
    "    return center_latitude, center_longitude\n",
    "\n",
    "\n",
    "def generate_80_cell_grid() -> pd.DataFrame:\n",
    "    \"\"\"Generate an 80-cell grid DataFrame with columns for southern, northern, western, eastern,\n",
    "    center_latitude, and center_longitude coordinates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The generated DataFrame.\n",
    "    \"\"\"\n",
    "    grid_data = []\n",
    "    \n",
    "    # Number of horizontal boxes in each band\n",
    "    # (proportional to the thickness of each band)\n",
    "    band_boxes = [4, 8, 12, 16]\n",
    "    \n",
    "    # Sines of latitudes\n",
    "    band_altitude = [1, 0.9, 0.7, 0.4, 0]\n",
    "\n",
    "    # Generate the 40 cells in the northern hemisphere\n",
    "    for band in range(len(band_boxes)):\n",
    "        n = band_boxes[band]\n",
    "        for i in range(n):\n",
    "            lats = 180 / math.pi * math.asin(band_altitude[band + 1])\n",
    "            latn = 180 / math.pi * math.asin(band_altitude[band])\n",
    "            lonw = -180 + 360 * float(i) / n\n",
    "            lone = -180 + 360 * float(i + 1) / n\n",
    "            box = (lats, latn, lonw, lone)\n",
    "            grid_data.append(box)\n",
    "\n",
    "    # Generate the 40 cells in the southern hemisphere by reversing the northern hemisphere cells\n",
    "    for box in grid_data[::-1]:\n",
    "        grid_data.append((-box[1], -box[0], box[2], box[3]))\n",
    "\n",
    "    # Create a DataFrame from the grid data\n",
    "    df = pd.DataFrame(grid_data, columns=['Southern', 'Northern', 'Western', 'Eastern'])\n",
    "\n",
    "    # Calculate center coordinates for each box and add them as new columns\n",
    "    center_coords = df.apply(calculate_center_coordinates, axis=1)\n",
    "    df[['Center_Latitude', 'Center_Longitude']] = pd.DataFrame(center_coords.tolist(), index=df.index)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def interpolate(x: float, y: float, p: float) -> float:\n",
    "    return y * p + (1 - p) * x\n",
    "\n",
    "\n",
    "def generate_8000_cell_grid(grid_80):\n",
    "\n",
    "    # Initialize an empty list to store subboxes\n",
    "    subbox_list = []\n",
    "\n",
    "    for index, row in grid_80.iterrows():\n",
    "        alts = math.sin(row['Southern'] * math.pi / 180)\n",
    "        altn = math.sin(row['Northern'] * math.pi / 180)\n",
    "\n",
    "        for y in range(10):\n",
    "            s = 180 * math.asin(interpolate(alts, altn, y * 0.1)) / math.pi\n",
    "            n = 180 * math.asin(interpolate(alts, altn, (y + 1) * 0.1)) / math.pi\n",
    "            for x in range(10):\n",
    "                w = interpolate(row['Western'], row['Eastern'], x * 0.1)\n",
    "                e = interpolate(row['Western'], row['Eastern'], (x + 1) * 0.1)\n",
    "\n",
    "                # Create a DataFrame for the subbox\n",
    "                subbox_df = pd.DataFrame({'Southern': [s], 'Northern': [n], 'Western': [w], 'Eastern': [e]})\n",
    "\n",
    "                # Append the subbox DataFrame to the list\n",
    "                subbox_list.append(subbox_df)\n",
    "\n",
    "    # Concatenate all subboxes into a single DataFrame\n",
    "    grid_8000 = pd.concat(subbox_list, ignore_index=True)\n",
    "\n",
    "    # Calculate center coordinates for each box and add them as new columns\n",
    "    center_coords = grid_8000.apply(calculate_center_coordinates, axis=1)\n",
    "    grid_8000[['Center_Latitude', 'Center_Longitude']] = pd.DataFrame(center_coords.tolist(), index=grid_8000.index)\n",
    "\n",
    "    # Calculate area of all 8000 cells\n",
    "    grid_8000['Area'] = grid_8000.apply(calculate_area, axis=1)\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    return grid_8000\n",
    "\n",
    "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the spherical distance (in kilometers) between two pairs of\n",
    "    latitude and longitude coordinates using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Spherical distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    radius: float = 6371.0  # Earth's mean radius\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat: float = lat2 - lat1\n",
    "    dlon: float = lon2 - lon1\n",
    "\n",
    "    a: float = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c: float = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance: float = radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def linearly_decreasing_weight(distance: float, max_distance: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a linearly decreasing weight based on the given distance\n",
    "    and maximum distance.\n",
    "\n",
    "    Args:\n",
    "        distance (float): The distance at which you want to calculate the weight.\n",
    "        max_distance (float): The maximum distance at which the weight becomes 0.\n",
    "\n",
    "    Returns:\n",
    "        float: The linearly decreasing weight, ranging from 1 to 0.\n",
    "    \"\"\"\n",
    "    # Ensure that distance is within the valid range [0, max_distance]\n",
    "    distance: float = max(0, min(distance, max_distance))\n",
    "\n",
    "    # Calculate the weight as a linear interpolation\n",
    "    weight: float = 1.0 - (distance / max_distance)\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def nearby_stations(grid_df, station_df):\n",
    "\n",
    "    # Initialize an empty list to store station IDs and weights as dictionaries\n",
    "    station_weights_within_radius = []\n",
    "\n",
    "    # Maximum distance for the weight calculation (e.g., 1200.0 km)\n",
    "    max_distance = 1200.0\n",
    "\n",
    "    # Use tqdm to track progress\n",
    "    for index, row in tqdm(grid_df.iterrows(), total=len(grid_df), desc=\"Processing\"):\n",
    "        center_lat = row['Center_Latitude']\n",
    "        center_lon = row['Center_Longitude']\n",
    "\n",
    "        # Calculate distances for each station in station_df\n",
    "        distances = station_df.apply(lambda x: haversine_distance(center_lat, center_lon, x['Latitude'], x['Longitude']), axis=1)\n",
    "\n",
    "        # Find station IDs within the specified radius\n",
    "        nearby_stations = station_df[distances <= max_distance]\n",
    "\n",
    "        # Calculate weights for each nearby station\n",
    "        weights = nearby_stations.apply(lambda x: linearly_decreasing_weight(distances[x.name], max_distance), axis=1)\n",
    "\n",
    "        # Create a dictionary of station IDs and weights\n",
    "        station_weights = dict(zip(nearby_stations['Station_ID'], weights))\n",
    "\n",
    "        # Append the dictionary to the result list\n",
    "        station_weights_within_radius.append(station_weights)\n",
    "\n",
    "    # Add the list of station IDs and weights as a new column\n",
    "    grid_df['Nearby_Stations'] = station_weights_within_radius\n",
    "\n",
    "    # Set index name\n",
    "    grid_df.index.name = 'Box_Number'\n",
    "    \n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf60964",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_80 = generate_80_cell_grid()\n",
    "grid_80['Area'] = grid_80.apply(calculate_area, axis=1)\n",
    "\n",
    "grid_8000 = generate_8000_cell_grid(grid_80)\n",
    "grid_8000['Area'] = grid_8000.apply(calculate_area, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be58d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_url = 'https://data.giss.nasa.gov/pub/gistemp/v4.inv'\n",
    "column_widths: List[int] = [11, 9, 10, 7, 3, 31]\n",
    "station_df: pd.DataFrame = pd.read_fwf(meta_url, widths=column_widths, header=None,\n",
    "                          names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'State', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_80 = nearby_stations(grid_80, station_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_8000 = nearby_stations(grid_8000, station_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febd168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590e861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76d6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_box_number(station_df, grid_80_df):\n",
    "    box_numbers = []\n",
    "\n",
    "    for _, station_row in tqdm(station_df.iterrows(), total=len(station_df)):\n",
    "        latitude = station_row['Latitude']\n",
    "        longitude = station_row['Longitude']\n",
    "\n",
    "        for box_number, box_row in grid_80_df.iterrows():\n",
    "            southern = box_row['Southern']\n",
    "            northern = box_row['Northern']\n",
    "            western = box_row['Western']\n",
    "            eastern = box_row['Eastern']\n",
    "\n",
    "            if southern <= latitude <= northern and western <= longitude <= eastern:\n",
    "                box_numbers.append(box_number)\n",
    "                break\n",
    "        else:\n",
    "            box_numbers.append(None)\n",
    "\n",
    "    return box_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e896722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find box numbers for each station, add to station_df\n",
    "box_numbers = find_box_number(station_df, grid_80)\n",
    "station_df['Box_Number'] = box_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851c677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d68251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abfc28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77402833",
   "metadata": {},
   "source": [
    "# Step 4: SST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping for now\n",
    "# Should be consolidated into step 0 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aaee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d24f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8be1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53979c49",
   "metadata": {},
   "source": [
    "# Step 5: Anomalyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalize_temperature_data(data, reference_period=(1951, 1980)):\n",
    "    # Extract the years from the DataFrame\n",
    "    years = data['Year'].unique()\n",
    "    \n",
    "    # Calculate monthly means for the reference period\n",
    "    reference_data = data[(data['Year'] >= reference_period[0]) & (data['Year'] <= reference_period[1])]\n",
    "    monthly_means = reference_data.iloc[:, 1:13].mean()\n",
    "    \n",
    "    # Initialize a DataFrame to store the anomalized data\n",
    "    anomalized_data = data.copy()\n",
    "    \n",
    "    # Anomalize each month's data\n",
    "    for month in tqdm(range(1, 13), desc=\"Anomalizing Months\"):\n",
    "        \n",
    "        # Calculate the anomaly for the current month\n",
    "        anomalized_data[f'Month_{month}'] = data.apply(lambda row: row[f'Month_{month}'] - monthly_means[month - 1], axis=1)\n",
    "    \n",
    "    return anomalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = step1_output\n",
    "df_anom = anomalize_temperature_data(df, reference_period=(1951, 1980))\n",
    "df_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec46b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116658f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d1a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25301e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce40100",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lons, lats, s=5, c='blue', alpha=0.5, marker='o')\n",
    "plt.grid()\n",
    "\n",
    "# Add contour lines based on the point density\n",
    "x_bins = np.linspace(-180, 180, 50)\n",
    "y_bins = np.linspace(-90, 90, 50)\n",
    "H, xedges, yedges = np.histogram2d(lons, lats, bins=(x_bins, y_bins))\n",
    "plt.contour(xedges[:-1], yedges[:-1], H.T, levels=10, colors='red', linewidths=1)\n",
    "\n",
    "# Customize labels and title\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Contour Map of Lat/Lon Pair Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density heatmap using plt.hist2d\n",
    "plt.hist2d(lons, lats, bins=250, cmap='viridis', vmin=0, vmax=10)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Density')\n",
    "\n",
    "# Customize labels and title\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Density Heatmap of Lat/Lon Pairs')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a Robinson projection\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.Robinson(central_longitude=0)})\n",
    "\n",
    "# Plot the latitude and longitude data in the Robinson projection\n",
    "ax.scatter(lons, lats, s=10, c='blue', alpha=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add coastlines for reference\n",
    "ax.coastlines()\n",
    "\n",
    "# Customize labels and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Lat/Lon Points in Robinson Projection')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558753f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
